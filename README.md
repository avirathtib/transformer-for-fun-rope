# GPT from Scratch with RoPE

A PyTorch implementation of a GPT-style transformer model built from scratch, featuring Rotary Position Embedding (RoPE) and trained on the FineWeb dataset.

## Architecture Overview

### Model Architecture (`model.py`)

**Key Components:**

- **Custom GPT Implementation**: Built from scratch with configurable parameters
- **Rotary Position Embedding (RoPE)**: Custom implementation for better positional encoding
- **Multi-Head Attention**: With RoPE integration for improved sequence understanding
- **Layer Normalization**: Pre-normalization architecture (RMSNorm-style)
- **GeLU Activation**: Custom implementation of Gaussian Error Linear Units

**Architecture Decisions:**

1. **RoPE over Learned Position Embeddings**: RoPE provides better extrapolation to longer sequences and relative position understanding
2. **Pre-LayerNorm**: Following modern transformer designs for better training stability
3. **Weight Sharing**: Input embeddings and output projection share weights (reduces parameters)
4. **Scaled Initialization**: Uses NanoGPT-style initialization with specific scaling for residual layers

**Model Configuration:**
- Vocabulary Size: 50,304 (GPT2 tokenizer)
- Embedding Dimension: 768
- Layers: 12
- Attention Heads: 12
- Dropout: 0.1
- Total Parameters: ~120M

### Training Pipeline (`train.py`)

**Features:**

- **Cosine Learning Rate Schedule**: With linear warmup for stable training
- **Gradient Clipping**: Prevents exploding gradients
- **Checkpointing**: Automatic model saving with resume capability
- **Mixed Evaluation**: Both perplexity and HellaSwag accuracy tracking
- **Weights & Biases Integration**: Comprehensive experiment tracking

**Training Configuration:**
- Initial Learning Rate: 6e-4
- Final Learning Rate: 6e-5 (10% of initial)
- Warmup Steps: 6% of total training steps
- Total Steps: 152,587 (equivalent to 1 epoch through FineWeb-Edu 10B tokens)
- Batch Size: 16
- Sequence Length: 1024 tokens
- Optimizer: AdamW (β₁=0.9, β₂=0.95, ε=1e-8)

### Data Processing (`data_processing_fineweb.py`)

**Custom Data Loading System:**

1. **FineWebDataset**: Memory-mapped NumPy array loading for efficiency
2. **FineWebDataLoader**: Streaming data loader with cross-file batching
3. **FineWebBatchIterator**: Iterator wrapper for epoch-based training

**Data Format:**
- Pre-tokenized text stored as NumPy arrays (.npy files)
- Each file contains ~50M tokens (200MB files)
- Training data: 100 shards in `edu_fineweb10B/`
- Validation data: 1 shard in `edu_fineweb10B_val/`

### Evaluation Systems

**Perplexity Evaluation:**
- Validates on held-out FineWeb-Edu data
- Cross-entropy loss on next-token prediction
- Tracks validation loss and perplexity metrics

**HellaSwag Evaluation (`hellaswag_eval.py`):**
- Commonsense reasoning benchmark
- Multiple-choice completion task
- Downloads dataset automatically
- Evaluates model's ability to predict most likely sentence completion

### Inference Engine (`inference.py`)

**Text Generation Features:**

- **Multiple Sampling Strategies**:
  - Greedy decoding (deterministic)
  - Top-k sampling
  - Nucleus (top-p) sampling
- **Temperature Control**: Adjusts randomness in generation
- **Repetition Penalty**: Reduces repetitive text generation
- **Configurable Generation Length**: Control output length

**Generation Presets:**
- `conservative`: Balanced quality/diversity (temp=0.6, top_k=40)
- `nucleus`: Creative generation (temp=0.8, top_p=0.9)
- `greedy`: Deterministic output

## Dataset: FineWeb-Edu

**Source**: Hugging Face FineWeb dataset (educational subset)
**Size**: ~10B tokens for training, ~100M tokens for validation
**Preprocessing**:
- Tokenized using GPT2 tokenizer
- Chunked into 1024-token sequences
- Stored as memory-mapped NumPy arrays for efficient loading

**Why FineWeb-Edu:**
1. **High Quality**: Filtered for educational content
2. **Scale**: Large enough for meaningful training
3. **Diversity**: Covers wide range of topics and writing styles
4. **Preprocessing**: Clean, deduplicated text

## Architecture Decisions Explained

### 1. Rotary Position Embedding (RoPE)

**Choice Rationale:**
- Better length extrapolation than learned position embeddings
- Relative position encoding improves attention patterns
- No additional parameters needed
- State-of-the-art performance in modern LLMs

**Implementation Details:**
- Applied to query and key vectors before attention computation
- Uses complex number representation for rotation
- Theta parameter set to 10,000 (following RoFormer paper)

### 2. Pre-LayerNorm Architecture

**Benefits:**
- More stable training compared to post-norm
- Better gradient flow through deep networks
- Follows GPT-3/GPT-4 architecture patterns

### 3. Weight Sharing (Embedding/Output)

**Rationale:**
- Reduces parameter count by ~13M parameters
- Common practice in modern language models
- Helps with training efficiency and generalization

### 4. Custom Data Loading

**Design Decisions:**
- Memory-mapped files for memory efficiency
- Cross-file batching for continuous token streams
- Separate validation data for unbiased evaluation

### 5. Evaluation Strategy

**Multi-metric Approach:**
- **Perplexity**: Direct measure of language modeling capability
- **HellaSwag**: Tests commonsense reasoning and completion ability
- **Both metrics**: Provide comprehensive model assessment

## Training Results

### Training Loss
![Training Loss](https://github.com/user-attachments/assets/36fcdb77-3a46-4003-b4b9-575593587768)

The training loss curve shows steady convergence over ~35,000 training steps, demonstrating effective learning on the FineWeb-Edu dataset.

### Validation Perplexity
![Validation Perplexity](https://github.com/user-attachments/assets/aea1d440-ed07-4e69-8918-c04cbdc57c7f)

Validation perplexity decreases consistently throughout training, indicating good generalization without overfitting.

## Performance Characteristics

**Training Speed:**
- ~120,000 tokens/second on single GPU
- Memory efficient due to gradient checkpointing considerations
- Scales with available compute

**Model Quality:**
- Comparable to similarly-sized models on HellaSwag
- Reasonable text generation quality
- Good performance on educational content


## Usage

### Training
```bash
python train.py --wandb_project "gpt-training" --wandb_run_name "experiment-1"
```

### Inference
```bash
python inference.py
```

## File Structure

```
├── model.py              # GPT architecture with RoPE
├── train.py              # Training loop with evaluation
├── inference.py          # Text generation interface
├── data_processing_fineweb.py  # Custom data loading
├── hellaswag_eval.py     # HellaSwag benchmark evaluation
├── edu_fineweb10B/       # Training data (100 shards)
├── edu_fineweb10B_val/   # Validation data (3 shards)
├── checkpoints/          # Model checkpoints
└── wandb/               # Experiment tracking logs
```

## Requirements

- PyTorch
- transformers (for GPT2 tokenizer)
- numpy
- tqdm
- wandb
- requests
- datasets

## Key Features

1. **From-Scratch Implementation**: Pure PyTorch, no external model libraries
2. **Modern Architecture**: RoPE, pre-norm, optimized initialization
3. **Efficient Training**: Memory-mapped data loading, gradient clipping, LR scheduling
4. **Comprehensive Evaluation**: Multiple metrics, automatic benchmarking
5. **Production Ready**: Checkpointing, logging, inference pipeline
6. **Educational Value**: Clean, well-documented code for learning transformer internals

This implementation serves as both a functional language model and an educational resource for understanding modern transformer architectures and training practices.
